# -*- coding: utf-8 -*-
"""fish.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14IIktmdv_XOM4wnsz4JLVQBsS-gHig19
"""

# installing the requirements
!pip install tensorflow opencv-python matplotlib --quiet

# importing the packages
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import pathlib

# adding the drive
from google.colab import drive
drive.mount('/content/drive')

data_dir = "/content/drive/MyDrive/Fish"
data_dir = pathlib.Path(data_dir)
print(data_dir)

dir_1 = "/content/drive/MyDrive/Fish/Dead"
dir_1 = pathlib.Path(dir_1)
image_files = list(dir_1.rglob("*.jpg")) + list(dir_1.rglob("*.png")) + list(dir_1.rglob("*.jpeg"))
print(len(image_files))


import cv2
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))

for i, img_path in enumerate(image_files[:6]):
    img = cv2.imread(str(img_path))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(2, 3, i+1)   # 2 rows, 3 columns
    plt.imshow(img)
    plt.title(f'Dead Fish Image {i+1}')

plt.tight_layout()
plt.show()

dir_2 = "/content/drive/MyDrive/Fish/Alive"
dir_2 = pathlib.Path(dir_2)
image_files = list(dir_2.rglob("*.jpg")) + list(dir_2.rglob("*.png")) + list(dir_2.rglob("*.jpeg"))
print(len(image_files))
import cv2
import matplotlib.pyplot as plt

plt.figure(figsize=(10,8))
for i, image_path in enumerate(image_files[:6]):
  img = cv2.imread(str(image_path))
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  plt.subplot(2, 3, i+1)   # 2 rows, 3 columns
  plt.imshow(img)
  plt.title(f'Alive Fish Image {i+1}')

plt.tight_layout()
plt.show()

img_size=(224,224)
batch_size = 16
seed = 42

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    labels='inferred',
    label_mode = 'binary',
    validation_split = 0.2,
    subset = "training",
    seed = seed,
    image_size = img_size,
    batch_size = batch_size
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    labels='inferred',
    label_mode='binary',
    validation_split=0.2,
    subset="validation",
    seed =seed,
    image_size = img_size,
    batch_size = batch_size
)
class_names = train_ds.class_names
print(class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)

data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.08),
    layers.RandomZoom(0.08),
    layers.RandomContrast(0.08),
])

from tensorflow.keras import Input

cnn_model = keras.Sequential([
    Input(shape=(224, 224, 3)),         # <<< Specify input shape here explicitly
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])


cnn_model.compile(
    optimizer = 'adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

cnn_model.summary()

checkpoint = keras.callbacks.ModelCheckpoint(
    "best_cnn_model.h5",
    save_best_only=True,
    monitor="val_accuracy",
    mode="max"
)

history = cnn_model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=[checkpoint]
)

basemodel = keras.applications.EfficientNetB0(
    include_top=False,
    input_shape=(224,224,3),
    weights='imagenet'
)

basemodel.trainable = False

# Freeze all except last N layers (e.g., last 20 layers)
for layer in basemodel.layers[:-20]:
    layer.trainable = False

model = keras.Sequential([
    layers.InputLayer(input_shape=(224,224,3)),
    data_augmentation,
    layers.Rescaling(1./255),
    basemodel,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

history_transfer_model=model.fit(
    train_ds,
    validation_data = val_ds,
    epochs=20
)

plt.figure(figsize=(8,4))
plt.plot(history.history['accuracy'], label='CNN train')
plt.plot(history.history['val_accuracy'], label='CNN val')
plt.plot(history_transfer_model.history['accuracy'], label='Transfer Train')
plt.plot(history_transfer_model.history['val_accuracy'], label='Transfer Val')

plt.title("CNN vs Transfer Learning Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

history
history_transfer_model

plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='CNN Train')
plt.plot(history.history['val_loss'], label='CNN Val')

plt.plot(history_transfer_model.history['loss'], label='Transfer Train')
plt.plot(history_transfer_model.history['val_loss'], label='Transfer Val')

plt.title("CNN vs Transfer Learning Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

import tensorflow as tf
import pathlib

data_dir = pathlib.Path("/content/drive/MyDrive/Fish")  # folder path
img_size = (224, 224)
batch_size = 8

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=42,
    image_size=img_size,
    batch_size=batch_size
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=42,
    image_size=img_size,
    batch_size=batch_size
)

train_ds = train_ds.prefetch(buffer_size=32)
val_ds = val_ds.prefetch(buffer_size=32)

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
])

inputs = tf.keras.Input(shape=(224, 224, 3))

x = data_augmentation(inputs)
x = tf.keras.applications.efficientnet.preprocess_input(x)

base_model = tf.keras.applications.EfficientNetB0(
    include_top=False,
    weights="imagenet",
    input_shape=(224,224,3)
)

base_model.trainable = False  # freeze full model

x = base_model(x, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.3)(x)
x = tf.keras.layers.Dense(256, activation="relu")(x)
x = tf.keras.layers.Dropout(0.3)(x)

outputs = tf.keras.layers.Dense(1, activation="sigmoid")(x)

model = tf.keras.Model(inputs, outputs)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50
)

base_model.trainable = True

for layer in base_model.layers[:-20]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

history_finetune = model.fit(train_ds, val_ds, epochs=20)

model.save("fish_alive_dead_model.keras")

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

import tensorflow as tf

model = tf.keras.models.load_model("/content/fish_alive_dead_model.keras")
print("Model loaded successfully!")

import numpy as np
from tensorflow.keras.preprocessing import image

img_size = 224  # same size used during training

def predict_fish(img_path):
    # Load image
    img = image.load_img(img_path, target_size=(img_size, img_size))

    # Convert to array
    img_array = image.img_to_array(img)

    # Expand dims â†’ (1, 224, 224, 3)
    img_array = np.expand_dims(img_array, axis=0)

    # Normalize
    img_array = img_array / 255.0

    # Predict
    pred = model.predict(img_array)[0][0]

    # Threshold 0.5 for binary classification
    if pred >= 0.5:
        print(f"Prediction: ðŸŸ¢ ALIVE ({pred:.2f})")
    else:
        print(f"Prediction: ðŸ”´ DEAD ({pred:.2f})")

from google.colab import files
imge = files.upload()

predict_fish("Screenshot 2025-12-12 120717.png")
predict_fish("Screenshot 2025-12-12 120728.png")
predict_fish("Screenshot 2025-12-12 120738.png")
predict_fish("Screenshot 2025-12-12 122235.png")
predict_fish("Screenshot 2025-12-12 122241.png")

import numpy as np
from tensorflow.keras.preprocessing import image

img_size = 224  # same size used during training

def predict_fish(img_path):
    # Load image
    img = image.load_img(img_path, target_size=(img_size, img_size))

    # Convert to array
    img_array = image.img_to_array(img)

    # Expand dims â†’ (1, 224, 224, 3)
    img_array = np.expand_dims(img_array, axis=0)

    # Normalize
    img_array = img_array / 255.0

    # Predict
    pred = model.predict(img_array)[0][0]

    # Threshold 0.5 for binary classification
    if pred >= 0.5:
        print(f"Prediction: ðŸŸ¢ ALIVE ({pred:.2f})")
    else:
        print(f"Prediction: ðŸ”´ DEAD ({pred:.2f})")

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
import numpy as np



data_dir = "/content/drive/MyDrive/Fish"   # only alive & dead inside
img_size = 224
batch_size = 32

datagen = ImageDataGenerator(
    rescale=1/255.0,
    validation_split=0.2   # 20% for validation
)

train_data = datagen.flow_from_directory(
    data_dir,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode="categorical",
    subset="training"
)

val_data = datagen.flow_from_directory(
    data_dir,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode="categorical",
    subset="validation"
)


model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),

    Dense(train_data.num_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=20
)

model.save("fish_detector_model.keras")
print("Model saved successfully!")

model = load_model("fish_detector_model.keras")
print("Model loaded!")

import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

def predict_image(path):
    img = load_img(path, target_size=(224, 224))
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    pred = model.predict(img_array)
    idx = np.argmax(pred)

    labels = list(train_data.class_indices.keys())

    print(f"Prediction: {labels[idx]}  ({pred[0][idx]:.2f})")

    # Display image
    plt.imshow(load_img(path))
    plt.axis("off")
    plt.show()


# Example:
predict_image("Screenshot 2025-12-12 120717.png")
predict_image("Screenshot 2025-12-12 120728.png")
predict_image("Screenshot 2025-12-12 120738.png")
predict_image("Screenshot 2025-12-12 122235.png")
predict_image("Screenshot 2025-12-12 122241.png")